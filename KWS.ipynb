{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_morph_dict(filepath):\n",
    "    with open (filepath, \"r\") as myfile:\n",
    "        data=myfile.readlines()\n",
    "    morph_dict = {}\n",
    "    for line in data:\n",
    "        line_split= line.split()\n",
    "        morph_dict[line_split[0].lower()]=line_split[1:]\n",
    "    return morph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(filepath, average=False, morph=False):\n",
    "    if (morph==True):\n",
    "        index_morph = read_morph_dict(\"../lib/dicts/morph.dct\")\n",
    "        \n",
    "    with open (filepath, \"r\") as myfile:\n",
    "        data=myfile.readlines()\n",
    "\n",
    "    #create file dictionary to contain data entries for each audio file\n",
    "    file_dict = {}\n",
    "    for line in data:\n",
    "        filename, channel, startTime, duration, token, score_str=line.split()\n",
    "        score = float(score_str)\n",
    "        token = token.lower()\n",
    "        \n",
    "        #if morph=true, replace by morphological decomposition\n",
    "        token_list=[]\n",
    "        if (morph==True and token in index_morph):\n",
    "            morph_token_list = index_morph[token]\n",
    "            word_dur = float(duration)/len(morph_token_list)\n",
    "            #if averaging, score remains same.\n",
    "            #if multiplying, decomposition score = score^1/n, where n = number of morphemes.\n",
    "            if (not average):\n",
    "                score = score ** (1.0/len(morph_token_list))\n",
    "            for n, morph_token in enumerate(morph_token_list):\n",
    "                token_list.append([morph_token, int(channel), float(startTime) + (n * word_dur), word_dur, float(score)])\n",
    "        else:\n",
    "            token_list = [[token, int(channel), float(startTime), float(duration), float(score)]]\n",
    "            \n",
    "            \n",
    "        if filename not in file_dict:\n",
    "            file_dict[filename] = []\n",
    "        file_dict[filename] = file_dict[filename] + token_list\n",
    "        #print file_dict[filename]\n",
    "\n",
    "    #index stores words and phrases as keys\n",
    "    indexed_dict = {}\n",
    "    for audio_file in file_dict:\n",
    "        t1=0\n",
    "        prev_word_end = 0\n",
    "        phrase_list = []\n",
    "        for word in file_dict[audio_file]:\n",
    "\n",
    "            #new word start time\n",
    "            new_word_start = word[2]\n",
    "\n",
    "            #if new word start time < previous word end time + 0.5, add current word to all phrases, else clear list\n",
    "            if (new_word_start < prev_word_end + 0.5):\n",
    "                for phrase in phrase_list:\n",
    "                    phrase[0] = \"{} {}\".format(phrase[0],word[0])\n",
    "                    \n",
    "                    if (average):\n",
    "                        n_words = len(phrase[0].split())\n",
    "                        phrase[4] = (word[4] + phrase[4]*(n_words-1))/n_words#weighted average of scores\n",
    "                    else:\n",
    "                        phrase[4] = phrase[4] * word[4] #multiply probabilities\n",
    "                        \n",
    "                    #phrase[4] = max(phrase[4],word[4])#maximum score\n",
    "                    #phrase[4] = min(1.0,phrase[4]+word[4])#sum and clip\n",
    "                    \n",
    "                    phrase[3] = round(word[2] - phrase[2] + word[3],5)#phrase duration = word start time - phrase start time + word duration\n",
    "                    #print (phrase[3], word[2], phrase[2], word[3])\n",
    "            else:\n",
    "                phrase_list = []\n",
    "\n",
    "            #add current word to list\n",
    "            phrase_list.append(word)\n",
    "\n",
    "            #add all phrases to indexed_dict\n",
    "            for phrase in phrase_list:\n",
    "                if phrase[0] not in indexed_dict:\n",
    "                    indexed_dict[phrase[0]] = [[audio_file] + phrase[1:5]]\n",
    "                else:\n",
    "                    indexed_dict[phrase[0]].append([audio_file] + phrase[1:5])\n",
    "\n",
    "            #store current word end time for next comparison\n",
    "            prev_word_end = word[2] + word[3]\n",
    "            \n",
    "            #check, words are sorted by timeStart\n",
    "            t2 = word[2]\n",
    "            #if t2<t1:\n",
    "            #    print prev_audio_file==audio_file\n",
    "            #    print t1_s, word\n",
    "            #    print \"WARNING: WORDS NOT IN CHRONOLOGICAL ORDER\"\n",
    "            t1 = word[2]\n",
    "            t1_s = word\n",
    "            prev_audio_file = audio_file\n",
    "    return indexed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML Parser for Query XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class query:\n",
    "    def __init__(self, kwid='', text='', orig_text=''):\n",
    "        self.kwid=kwid\n",
    "        self.text=text\n",
    "        self.orig_text=orig_text\n",
    "        self.oov=0\n",
    "        self.search_time=0.0\n",
    "        self.res_list=[]\n",
    "        self.system_count = 0\n",
    "        self.normalized=False\n",
    "        \n",
    "        self.T=36000\n",
    "        self.beta=999.9\n",
    "    \n",
    "    def print_variables(self):\n",
    "        return (self.kwid, self.orig_text, self.oov, self.search_time, self.res_list)\n",
    "        \n",
    "    def print_output(self):\n",
    "        output_xml = []\n",
    "        output_xml.append(\"<detected_kwlist kwid=\\\"{}\\\" oov_count=\\\"{}\\\" search_time=\\\"{:.10f}\\\">\".format(self.kwid, self.oov, self.search_time))\n",
    "        for res in self.res_list:\n",
    "            output_xml.append(\"<kw file=\\\"{}\\\" channel=\\\"{}\\\" tbeg=\\\"{:0.2f}\\\" dur=\\\"{:0.3f}\\\" score=\\\"{:0.6f}\\\" decision=\\\"YES\\\"/>\".format(res[0], res[1], res[2], res[3], res[4]))\n",
    "        output_xml.append(\"</detected_kwlist>\") \n",
    "        return output_xml\n",
    "    \n",
    "    def normalize(self, gamma):#ex4\n",
    "        if(self.normalized):\n",
    "            print(\"Already Normalized\")\n",
    "        self.gamma = gamma\n",
    "        sum_denom=0\n",
    "        for res in self.res_list:\n",
    "            sum_denom+=res[4]**self.gamma\n",
    "        for res in self.res_list:\n",
    "            res[4] = (res[4]**gamma)/sum_denom\n",
    "        self.normalized=True\n",
    "        \n",
    "    def kst(self):\n",
    "        N_true = len(self.res_list)\n",
    "        threshold = self.beta * N_true / (self.T + (self.beta-1) * N_true)\n",
    "        new_list = []\n",
    "        for res in self.res_list:\n",
    "            #print (res[4],self.threshold, res[4]>self.threshold)\n",
    "            if (res[4]>threshold):\n",
    "                new_list.append(res)\n",
    "        self.res_list = new_list\n",
    "    \n",
    "    def kst2(self):\n",
    "        total_score = 0.0\n",
    "        alpha=1.5\n",
    "        for res in self.res_list:\n",
    "            total_score += res[4]\n",
    "        threshold = self.beta * total_score * alpha / (self.T + (self.beta-1) * total_score * alpha)\n",
    "        new_list = []\n",
    "        for res in self.res_list:\n",
    "            #print (res[4],self.threshold, res[4]>self.threshold)\n",
    "            if (res[4]>threshold):\n",
    "                new_list.append(res)\n",
    "        #print total_score, threshold\n",
    "        self.res_list = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "def get_queries(filepath, morph=False):\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    search_list = []\n",
    "    if (morph==True):\n",
    "        query_morph = read_morph_dict(\"../lib/dicts/morph.kwslist.dct\")\n",
    "        for child in root:\n",
    "            ctext_items = child[0].text.split()\n",
    "            ctext_morph = []\n",
    "            for ctext in ctext_items:\n",
    "                if ctext in query_morph:\n",
    "                    ctext_morph.append(\" \".join(query_morph[ctext]))\n",
    "                else:\n",
    "                    ctext_morph.append(ctext)\n",
    "            search_list.append(query(child.attrib['kwid'], \" \".join(ctext_morph), child[0].text))\n",
    "    else:\n",
    "        for child in root:\n",
    "            search_list.append(query(child.attrib['kwid'], child[0].text, child[0].text))\n",
    "    return search_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kst_list(search_list):\n",
    "    new_list = []\n",
    "    for q in search_list:\n",
    "        #print q.print_variables()\n",
    "        q.kst()\n",
    "        new_list.append(q)\n",
    "    return new_list\n",
    "\n",
    "def kst2_list(search_list):\n",
    "    new_list = []\n",
    "    for q in search_list:\n",
    "        #print q.print_variables()\n",
    "        q.kst2()\n",
    "        new_list.append(q)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_list(search_list, gamma=1):\n",
    "    new_list = []\n",
    "    for q in search_list:\n",
    "        #print q.print_variables()\n",
    "        new_list.append(q.normalize(gamma))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer  \n",
    "\n",
    "def search_index(search_list, indexed_dict, gamma=0): \n",
    "    oov_count=0\n",
    "    for q in search_list:\n",
    "        start = timer()\n",
    "        if q.text in indexed_dict:\n",
    "            q.res_list = indexed_dict[q.text]\n",
    "        else:\n",
    "            oov_count+=1\n",
    "        end = timer()\n",
    "        q.oov = 1\n",
    "        if (gamma>0):\n",
    "            q.normalize(gamma)\n",
    "        q.search_time = end - start\n",
    "    print \"oov_count={}\".format(oov_count)\n",
    "    return search_list\n",
    "\n",
    "def gen_output(search_results, output_file):\n",
    "    kw_top = \"<kwslist kwlist_filename=\\\"IARPA-babel202b-v1.0d_conv-dev.kwlist.xml\\\" language=\\\"swahili\\\" system_id=\\\"\\\">\"\n",
    "    kw_end = \"</kwslist>\"\n",
    "    output_xml = []\n",
    "    output_xml.append(kw_top)\n",
    "    for search in search_results:\n",
    "        output_xml = output_xml + search.print_output()\n",
    "    output_xml.append(kw_end)\n",
    "    with open(output_file, \"w\") as text_file:\n",
    "        text_file.write(\"\\n\".join(output_xml))\n",
    "    return output_xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Easier Scoring Interface Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_scores(res_name, print_latex=False):\n",
    "    \n",
    "    subprocess.call([\"rm\", \"-r\", \"../scoring/{}/\".format(res_name)])\n",
    "    subprocess.call([\"../scripts/score.sh\", \"../output/{}.xml\".format(res_name), \"../scoring\"])\n",
    "    results = {}\n",
    "    \n",
    "    for res_type in [\"all\", \"iv\", \"oov\"]:\n",
    "        p = subprocess.Popen([\"../scripts/termselect.sh\", \"../lib/terms/ivoov.map\", \"../output/{}.xml\".format(res_name), \"../scoring\", res_type], stdout=subprocess.PIPE)\n",
    "        output_all, err = p.communicate()\n",
    "        results[res_type] = (float(output_all.split(\" \")[1].split(\"=\")[1]), float(output_all.split(\" \")[2].split(\"=\")[1]), int(output_all.split(\" \")[3].split(\"=\")[1].replace(\"\\n\",\"\")))\n",
    "        if (not print_latex):\n",
    "            print (\"{} ({}) - TWV:{:1.4f}\".format(res_type, results[res_type][2], results[res_type][0]))\n",
    "    for res_type in [\"short\", \"long\"]:\n",
    "        p = subprocess.Popen([\"../scripts/termselect.sh\", \"../lib/terms/longshort.map\", \"../output/{}.xml\".format(res_name), \"../scoring\", res_type], stdout=subprocess.PIPE)\n",
    "        output_all, err = p.communicate()\n",
    "        results[res_type] = (float(output_all.split(\" \")[1].split(\"=\")[1]), float(output_all.split(\" \")[2].split(\"=\")[1]), int(output_all.split(\" \")[3].split(\"=\")[1].replace(\"\\n\",\"\")))\n",
    "        if (not print_latex):\n",
    "            print (\"{} ({}) - TWV:{:1.4f}\".format(res_type, results[res_type][2], results[res_type][0]))\n",
    "    for res_type in [\"word\", \"phrase\"]:\n",
    "        p = subprocess.Popen([\"../scripts/termselect.sh\", \"../lib/terms/phraseword.map\", \"../output/{}.xml\".format(res_name), \"../scoring\", res_type], stdout=subprocess.PIPE)\n",
    "        output_all, err = p.communicate()\n",
    "        results[res_type] = (float(output_all.split(\" \")[1].split(\"=\")[1]), float(output_all.split(\" \")[2].split(\"=\")[1]), int(output_all.split(\" \")[3].split(\"=\")[1].replace(\"\\n\",\"\")))\n",
    "        if (not print_latex):\n",
    "            print (\"{} ({}) - TWV:{:1.4f}\".format(res_type, results[res_type][2], results[res_type][0]))\n",
    "        \n",
    "    p=subprocess.Popen([\"grep\", \"Summary\", \"../scoring/{}/Full-Occur-MITLLFA3-AppenWordSeg.bsum.txt\".format(res_name)], stdout= subprocess.PIPE)\n",
    "    if (not print_latex):\n",
    "        print (\"Threshold: {}\".format(results['all'][1]))\n",
    "    \n",
    "    output, err = p.communicate()\n",
    "    Targ = int(output.split(\"|\")[4])\n",
    "    Corr = int(output.split(\"|\")[5])\n",
    "    FA = int(output.split(\"|\")[6])\n",
    "    Miss = int(output.split(\"|\")[7])\n",
    "    results[\"targets\"]=Targ\n",
    "    results[\"correct\"]=Corr\n",
    "    results[\"false_alarm\"]=FA\n",
    "    results[\"missed\"]=Miss\n",
    "    if (not print_latex):\n",
    "        print (\"Targets: {} Correct: {} False Alarms: {} Miss: {}\".format(Targ,Corr,FA,Miss))\n",
    "    \n",
    "    #latex printing\n",
    "    if (print_latex):\n",
    "        print (\"{:1.3f} & {:1.3f} & {:1.3f} & {:1.3f} & {:1.3f} & {:1.3f} & {:1.3f} & {:1.3f}\\\\\\\\\\hline\".format(results['all'][0],results['iv'][0],results['oov'][0],results['short'][0],results['long'][0],results['phrase'][0],results['word'][0],results['all'][1]))\n",
    "        print (\"{} & {} & {} & {}\\\\\\\\\\hline\".format(Targ,Corr,FA,Miss))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Length Map - >10 char = long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "phrase_ct = 1\n",
    "word_ct = 1\n",
    "print_list = []\n",
    "for q in query_list:\n",
    "    if (\" \" in q.text):\n",
    "        print_list.append(\"phrase {} {:04d}\".format(q.kwid[6:],phrase_ct))\n",
    "        phrase_ct+=1\n",
    "    else:\n",
    "        print_list.append(\"word {} {:04d}\".format(q.kwid[6:],word_ct))\n",
    "        word_ct+=1\n",
    "#print \"\\n\".join(print_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "long_ct = 1\n",
    "short_ct = 1\n",
    "print_list = []\n",
    "for q in query_list:\n",
    "    if (len(q.text)>10):\n",
    "        print_list.append(\"long {} {:04d}\".format(q.kwid[6:],long_ct))\n",
    "        long_ct+=1\n",
    "    else:\n",
    "        print_list.append(\"short {} {:04d}\".format(q.kwid[6:],short_ct))\n",
    "        short_ct+=1\n",
    "#print \"\\n\".join(print_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex1\n",
    "\n",
    "#### Reference KWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Entries: 315726 | Index Build Time: 4.66487288475\n",
      "oov_count=8\n",
      "all (488) - TWV:1.0000\n",
      "iv (388) - TWV:1.0000\n",
      "oov (100) - TWV:1.0000\n",
      "short (321) - TWV:1.0000\n",
      "long (167) - TWV:1.0000\n",
      "word (288) - TWV:1.0000\n",
      "phrase (200) - TWV:1.0000\n",
      "Threshold: 1.0\n",
      "Targets: 963 Correct: 963 False Alarms: 0 Miss: 0\n",
      "Time Taken: 0.000944852828979\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "kw_idx_ref = create_index(\"../lib/ctms/reference.ctm\", False, False)\n",
    "end = timer()\n",
    "print(\"Index Entries: {} | Index Build Time: {}\".format(len(kw_idx_ref.keys()),end - start))\n",
    "\n",
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "\n",
    "start = timer()\n",
    "reference_results = search_index(query_list, kw_idx_ref)\n",
    "end = timer()\n",
    "\n",
    "_ = gen_output(reference_results, \"../output/reference.xml\")\n",
    "_ = get_scores(\"reference\")\n",
    "print (\"Time Taken: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2\n",
    "\n",
    "#### Word-based KWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Entries: 263707 | Index Build Time: 2.51817297935\n",
      "oov_count=246\n",
      "all (488) - TWV:0.3189\n",
      "iv (388) - TWV:0.4011\n",
      "oov (100) - TWV:0.0000\n",
      "short (321) - TWV:0.3123\n",
      "long (167) - TWV:0.3317\n",
      "word (288) - TWV:0.3221\n",
      "phrase (200) - TWV:0.3145\n",
      "Threshold: 0.043\n",
      "Targets: 963 Correct: 405 False Alarms: 320 Miss: 558\n",
      "Time Taken: 0.00333786010742\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "kw_idx_word = create_index(\"../lib/ctms/decode.ctm\", False, False)\n",
    "end = timer()\n",
    "print(\"Index Entries: {} | Index Build Time: {}\".format(len(kw_idx_word.keys()),end - start))\n",
    "\n",
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "\n",
    "start = timer()\n",
    "word_results = search_index(query_list, kw_idx_word)\n",
    "end = timer()\n",
    "\n",
    "_ = gen_output(word_results, \"../output/decode.xml\")\n",
    "_ = get_scores(\"decode\")\n",
    "print (\"Time Taken: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#query morph\n",
    "! head ../lib/dicts/morph.kwslist.dct"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#1-best index morph\n",
    "! head ../lib/dicts/morph.dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Decomposed Decode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Entries: 785968 | Index Build Time: 9.23704099655\n",
      "oov_count=251\n",
      "all (488) - TWV:0.3183\n",
      "iv (388) - TWV:0.3828\n",
      "oov (100) - TWV:0.0678\n",
      "short (321) - TWV:0.3265\n",
      "long (167) - TWV:0.3025\n",
      "word (288) - TWV:0.3397\n",
      "phrase (200) - TWV:0.2875\n",
      "Threshold: 0.301\n",
      "Targets: 963 Correct: 410 False Alarms: 543 Miss: 553\n",
      "Time Taken: 0.00819802284241\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "kw_idx_morph = create_index(\"../lib/ctms/decode-morph.ctm\", True, False)\n",
    "end = timer()\n",
    "print(\"Index Entries: {} | Index Build Time: {}\".format(len(kw_idx_morph.keys()),end - start))\n",
    "\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "\n",
    "start = timer()\n",
    "query_results_morph = search_index(query_list_morph, kw_idx_morph)\n",
    "end = timer()\n",
    "\n",
    "_ = gen_output(query_results_morph, \"../output/decode_morph.xml\")\n",
    "_ = get_scores(\"decode_morph\")\n",
    "print (\"Time Taken: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dict-based Decomposition Decode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Entries: 790910 | Index Build Time: 13.8114390373\n",
      "oov_count=238\n",
      "all (488) - TWV:0.3156\n",
      "iv (388) - TWV:0.3921\n",
      "oov (100) - TWV:0.0189\n",
      "short (321) - TWV:0.3064\n",
      "long (167) - TWV:0.3333\n",
      "word (288) - TWV:0.3152\n",
      "phrase (200) - TWV:0.3163\n",
      "Threshold: 0.205\n",
      "Targets: 963 Correct: 417 False Alarms: 602 Miss: 546\n",
      "Time Taken: 0.00692296028137\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "kw_idx_morph_manual = create_index(\"../lib/ctms/decode.ctm\", True, True)\n",
    "end = timer()\n",
    "print(\"Index Entries: {} | Index Build Time: {}\".format(len(kw_idx_morph_manual.keys()),end - start))\n",
    "\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "\n",
    "start = timer()\n",
    "query_results_morph_manual = search_index(query_list_morph, kw_idx_morph_manual)\n",
    "end = timer()\n",
    "\n",
    "_ = gen_output(query_results_morph_manual, \"../output/decode_morph_manual.xml\")\n",
    "_ = get_scores(\"decode_morph_manual\")\n",
    "print (\"Time Taken: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word-Based Decode with STO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oov_count=246\n",
      "0.320 & 0.402 & 0.000 & 0.313 & 0.332 & 0.315 & 0.323 & 0.167\\\\\\hline\n",
      "963 & 405 & 320 & 558\\\\\\hline\n",
      "Time Taken: 0.00280213356018\n"
     ]
    }
   ],
   "source": [
    "kw_idx_word = create_index(\"../lib/ctms/decode.ctm\", True, False)\n",
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "\n",
    "start = timer()\n",
    "query_results_sto = search_index(query_list, kw_idx_word)\n",
    "#kst2_list(query_results_sto)\n",
    "end = timer()\n",
    "\n",
    "_ = gen_output(query_results_sto, \"../output/decode_sto.xml\")\n",
    "_ = get_scores(\"decode_sto\", True)\n",
    "print (\"Time Taken: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Decomposed Decode with STO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oov_count=251\n",
      "0.326 & 0.392 & 0.068 & 0.335 & 0.307 & 0.291 & 0.350 & 0.048\\\\\\hline\n",
      "963 & 395 & 294 & 568\\\\\\hline\n",
      "Time Taken: 0.00640797615051\n"
     ]
    }
   ],
   "source": [
    "keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\", True, False)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "\n",
    "start = timer()\n",
    "query_results_morph_sto = search_index(query_list_morph, keyword_index_morph)\n",
    "normalize_list(query_results_morph_sto,1)\n",
    "kst2_list(query_results_morph_sto)\n",
    "end = timer()\n",
    "\n",
    "_ = gen_output(query_results_morph_sto, \"../output/decode_morph_sto.xml\")\n",
    "_ = get_scores(\"decode_morph_sto\", True)\n",
    "print (\"Time Taken: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dict-based Decomposition with STO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oov_count=238\n",
      "0.316 & 0.392 & 0.019 & 0.306 & 0.333 & 0.316 & 0.315 & 0.205\\\\\\hline\n",
      "963 & 417 & 602 & 546\\\\\\hline\n",
      "Time Taken: 0.0039119720459\n"
     ]
    }
   ],
   "source": [
    "keyword_index_morph_manual = create_index(\"../lib/ctms/decode.ctm\", True, True)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "\n",
    "start = timer()\n",
    "query_results_morph_manual_sto = search_index(query_list_morph, keyword_index_morph_manual)\n",
    "#kst2_list(query_results_morph_manual_sto)\n",
    "end = timer()\n",
    "\n",
    "_ = gen_output(query_results_morph_manual_sto, \"../output/decode_morph_manual_sto.xml\")\n",
    "_ = get_scores(\"decode_morph_manual_sto\", True)\n",
    "print (\"Time Taken: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gamma Experimentation\n",
    "gamma_list = [(x)/10.0 for x in range (0,51,5)]\n",
    "res_types=['all','iv','oov','long','short','phrase','word']\n",
    "\n",
    "#Word Avg\n",
    "res_dict_word_mult = {}\n",
    "for res_type in res_types:\n",
    "    res_dict_word_mult[res_type] = []\n",
    "res_dict_word_mult['threshold']=[]\n",
    "\n",
    "for i,gamma in enumerate(gamma_list):\n",
    "    kw_idx_word = create_index(\"../lib/ctms/decode.ctm\", False, False)\n",
    "    query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "    query_results_sto = search_index(query_list, kw_idx_word)\n",
    "    print gamma\n",
    "    normalize_list(query_results_sto,gamma)\n",
    "    _ = gen_output(query_results_sto, \"../output/decode_sto.xml\")\n",
    "    res = get_scores(\"decode_sto\")\n",
    "    \n",
    "    for res_type in res_types:\n",
    "        res_dict_word_mult[res_type].append(res[res_type][0])\n",
    "    res_dict_word_mult['threshold'].append(res['all'][1])\n",
    "        \n",
    "#Word Mult\n",
    "res_dict_word_avg = {}\n",
    "for res_type in res_types:\n",
    "    res_dict_word_avg[res_type] = []\n",
    "res_dict_word_avg['threshold']=[]\n",
    "    \n",
    "for i,gamma in enumerate(gamma_list):\n",
    "    kw_idx_word = create_index(\"../lib/ctms/decode.ctm\", True, False)\n",
    "    query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "    query_results_sto = search_index(query_list, kw_idx_word)\n",
    "    print gamma\n",
    "    normalize_list(query_results_sto,gamma)\n",
    "    _ = gen_output(query_results_sto, \"../output/decode_sto.xml\")\n",
    "    res = get_scores(\"decode_sto\")\n",
    "    \n",
    "    for res_type in res_types:\n",
    "        res_dict_word_avg[res_type].append(res[res_type][0])\n",
    "    res_dict_word_avg['threshold'].append(res['all'][1])\n",
    "        \n",
    "#Pre-decomposed Morph Mult\n",
    "res_dict_morph_mult = {}\n",
    "for res_type in res_types:\n",
    "    res_dict_morph_mult[res_type] = []\n",
    "res_dict_morph_mult['threshold']=[]\n",
    "\n",
    "for i,gamma in enumerate(gamma_list):\n",
    "    print gamma\n",
    "    keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\", False, False)\n",
    "    query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "    query_results_morph_sto = search_index(query_list_morph, keyword_index_morph)\n",
    "    normalize_list(query_results_morph_sto,gamma)\n",
    "    _ = gen_output(query_results_morph_sto, \"../output/decode_morph_sto.xml\")\n",
    "    res = get_scores(\"decode_morph_sto\")\n",
    "    \n",
    "    for res_type in res_types:\n",
    "        res_dict_morph_mult[res_type].append(res[res_type][0])\n",
    "    res_dict_morph_mult['threshold'].append(res['all'][1])\n",
    "        \n",
    "#Pre-decomposed Morph Avg\n",
    "res_dict_morph_avg = {}\n",
    "for res_type in res_types:\n",
    "    res_dict_morph_avg[res_type] = []\n",
    "res_dict_morph_avg['threshold']=[]\n",
    "\n",
    "for i,gamma in enumerate(gamma_list):\n",
    "    print gamma\n",
    "    keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\", True, False)\n",
    "    query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "    query_results_morph_sto = search_index(query_list_morph, keyword_index_morph)\n",
    "    normalize_list(query_results_morph_sto,gamma)\n",
    "    _ = gen_output(query_results_morph_sto, \"../output/decode_morph_sto.xml\")\n",
    "    res = get_scores(\"decode_morph_sto\")\n",
    "    \n",
    "    for res_type in res_types:\n",
    "        res_dict_morph_avg[res_type].append(res[res_type][0])\n",
    "    res_dict_morph_avg['threshold'].append(res['all'][1])\n",
    "        \n",
    "#Manual Morph Mult\n",
    "res_dict_morph_manual_mult = {}\n",
    "for res_type in res_types:\n",
    "    res_dict_morph_manual_mult[res_type] = []\n",
    "res_dict_morph_manual_mult['threshold']=[]\n",
    "\n",
    "for i,gamma in enumerate(gamma_list):\n",
    "    print gamma\n",
    "    keyword_index_morph_manual = create_index(\"../lib/ctms/decode.ctm\", False, True)\n",
    "    query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "    query_results_morph_manual_sto = search_index(query_list_morph, keyword_index_morph_manual)\n",
    "    normalize_list(query_results_morph_manual_sto,gamma)\n",
    "    _ = gen_output(query_results_morph_manual_sto, \"../output/decode_morph_manual_sto.xml\")\n",
    "    res = get_scores(\"decode_morph_manual_sto\")\n",
    "    \n",
    "    for res_type in res_types:\n",
    "        res_dict_morph_manual_mult[res_type].append(res[res_type][0])\n",
    "    res_dict_morph_manual_mult['threshold'].append(res['all'][1])\n",
    "        \n",
    "#Manual Morph Avg\n",
    "res_dict_morph_manual_avg = {}\n",
    "for res_type in res_types:\n",
    "    res_dict_morph_manual_avg[res_type] = []\n",
    "res_dict_morph_manual_avg['threshold']=[]\n",
    "\n",
    "for i,gamma in enumerate(gamma_list):\n",
    "    print gamma\n",
    "    keyword_index_morph_manual = create_index(\"../lib/ctms/decode.ctm\", True, True)\n",
    "    query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "    query_results_morph_manual_sto = search_index(query_list_morph, keyword_index_morph_manual)\n",
    "    normalize_list(query_results_morph_manual_sto,gamma)\n",
    "    _ = gen_output(query_results_morph_manual_sto, \"../output/decode_morph_manual_sto.xml\")\n",
    "    res = get_scores(\"decode_morph_manual_sto\")\n",
    "    \n",
    "    for res_type in res_types:\n",
    "        res_dict_morph_manual_avg[res_type].append(res[res_type][0])\n",
    "    res_dict_morph_manual_avg['threshold'].append(res['all'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morph_manual_mult_all = [0.3206, 0.3197, 0.3189, 0.3194, 0.3182, 0.3165, 0.3163, 0.3157, 0.3149, 0.3136, 0.3141];\n",
      "morph_manual_mult_iv = [0.3986, 0.3975, 0.3965, 0.3971, 0.3956, 0.3934, 0.3931, 0.3924, 0.3914, 0.3898, 0.3904];\n",
      "morph_manual_mult_oov = [0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018];\n",
      "morph_manual_mult_threshold = [0.038, 0.04, 0.043, 0.049, 0.038, 0.01, 0.007, 0.005, 0.001, 0.001, 0.0];\n"
     ]
    }
   ],
   "source": [
    "print_types=['all','iv','oov','threshold']\n",
    "hd_text = \"morph_manual_mult\"\n",
    "nres = res_dict_morph_manual_mult\n",
    "for tp in print_types:\n",
    "    print \"{}_{} = [{}];\".format(hd_text, tp, \", \".join([str(round(x,4)) for x in nres[tp]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oov_count=246\n",
      "0.319 & 0.401 & 0.000 & 0.312 & 0.332 & 0.314 & 0.322 & 0.043\\\\\\hline\n",
      "963 & 405 & 320 & 558\\\\\\hline\n",
      "oov_count=246\n",
      "0.320 & 0.402 & 0.000 & 0.313 & 0.332 & 0.315 & 0.323 & 0.167\\\\\\hline\n",
      "963 & 405 & 320 & 558\\\\\\hline\n",
      "oov_count=251\n",
      "0.318 & 0.383 & 0.068 & 0.327 & 0.303 & 0.287 & 0.340 & 0.061\\\\\\hline\n",
      "963 & 410 & 543 & 553\\\\\\hline\n",
      "oov_count=251\n",
      "0.318 & 0.383 & 0.068 & 0.326 & 0.303 & 0.287 & 0.340 & 0.301\\\\\\hline\n",
      "963 & 410 & 543 & 553\\\\\\hline\n",
      "oov_count=238\n",
      "0.314 & 0.390 & 0.018 & 0.304 & 0.333 & 0.316 & 0.313 & 0.043\\\\\\hline\n",
      "963 & 417 & 602 & 546\\\\\\hline\n",
      "oov_count=238\n",
      "0.316 & 0.392 & 0.019 & 0.306 & 0.333 & 0.316 & 0.315 & 0.205\\\\\\hline\n",
      "963 & 417 & 602 & 546\\\\\\hline\n",
      "oov_count=246\n",
      "0.320 & 0.403 & 0.000 & 0.314 & 0.331 & 0.314 & 0.324 & 0.114\\\\\\hline\n",
      "963 & 397 & 275 & 566\\\\\\hline\n",
      "oov_count=246\n",
      "0.320 & 0.403 & 0.000 & 0.315 & 0.332 & 0.314 & 0.325 & 0.167\\\\\\hline\n",
      "963 & 399 & 280 & 564\\\\\\hline\n",
      "oov_count=251\n",
      "0.321 & 0.386 & 0.068 & 0.330 & 0.303 & 0.288 & 0.343 & 0.061\\\\\\hline\n",
      "963 & 405 & 465 & 558\\\\\\hline\n",
      "oov_count=251\n",
      "0.320 & 0.384 & 0.068 & 0.328 & 0.303 & 0.287 & 0.342 & 0.301\\\\\\hline\n",
      "963 & 409 & 499 & 554\\\\\\hline\n",
      "oov_count=238\n",
      "0.317 & 0.394 & 0.018 & 0.308 & 0.334 & 0.317 & 0.317 & 0.114\\\\\\hline\n",
      "963 & 402 & 467 & 561\\\\\\hline\n",
      "oov_count=238\n",
      "0.317 & 0.394 & 0.018 & 0.308 & 0.334 & 0.317 & 0.317 & 0.173\\\\\\hline\n",
      "963 & 404 & 448 & 559\\\\\\hline\n",
      "oov_count=246\n",
      "0.320 & 0.402 & 0.000 & 0.313 & 0.332 & 0.315 & 0.323 & 0.020\\\\\\hline\n",
      "963 & 405 & 320 & 558\\\\\\hline\n",
      "oov_count=246\n",
      "0.320 & 0.402 & 0.000 & 0.313 & 0.332 & 0.314 & 0.323 & 0.029\\\\\\hline\n",
      "963 & 405 & 320 & 558\\\\\\hline\n",
      "oov_count=251\n",
      "0.326 & 0.393 & 0.068 & 0.336 & 0.307 & 0.291 & 0.350 & 0.057\\\\\\hline\n",
      "963 & 410 & 543 & 553\\\\\\hline\n",
      "oov_count=251\n",
      "0.326 & 0.392 & 0.068 & 0.335 & 0.307 & 0.291 & 0.350 & 0.048\\\\\\hline\n",
      "963 & 410 & 543 & 553\\\\\\hline\n",
      "oov_count=238\n",
      "0.319 & 0.396 & 0.018 & 0.311 & 0.334 & 0.317 & 0.320 & 0.043\\\\\\hline\n",
      "963 & 417 & 602 & 546\\\\\\hline\n",
      "oov_count=238\n",
      "0.320 & 0.398 & 0.018 & 0.311 & 0.338 & 0.320 & 0.320 & 0.056\\\\\\hline\n",
      "963 & 417 & 602 & 546\\\\\\hline\n",
      "oov_count=246\n",
      "0.320 & 0.402 & 0.000 & 0.314 & 0.331 & 0.314 & 0.324 & 0.033\\\\\\hline\n",
      "963 & 397 & 275 & 566\\\\\\hline\n",
      "oov_count=246\n",
      "0.320 & 0.403 & 0.000 & 0.314 & 0.332 & 0.314 & 0.324 & 0.033\\\\\\hline\n",
      "963 & 399 & 280 & 564\\\\\\hline\n",
      "oov_count=251\n",
      "0.326 & 0.393 & 0.068 & 0.336 & 0.307 & 0.291 & 0.350 & 0.054\\\\\\hline\n",
      "963 & 405 & 465 & 558\\\\\\hline\n",
      "oov_count=251\n",
      "0.326 & 0.392 & 0.068 & 0.335 & 0.307 & 0.291 & 0.350 & 0.047\\\\\\hline\n",
      "963 & 409 & 499 & 554\\\\\\hline\n",
      "oov_count=238\n",
      "0.318 & 0.395 & 0.018 & 0.310 & 0.334 & 0.317 & 0.319 & 0.025\\\\\\hline\n",
      "963 & 402 & 467 & 561\\\\\\hline\n",
      "oov_count=238\n",
      "0.320 & 0.398 & 0.018 & 0.311 & 0.338 & 0.320 & 0.320 & 0.062\\\\\\hline\n",
      "963 & 404 & 448 & 559\\\\\\hline\n",
      "oov_count=246\n",
      "0.296 & 0.372 & 0.000 & 0.278 & 0.330 & 0.306 & 0.288 & 0.115\\\\\\hline\n",
      "963 & 268 & 150 & 695\\\\\\hline\n",
      "oov_count=246\n",
      "0.295 & 0.371 & 0.000 & 0.277 & 0.331 & 0.307 & 0.287 & 0.121\\\\\\hline\n",
      "963 & 267 & 152 & 696\\\\\\hline\n",
      "oov_count=251\n",
      "0.299 & 0.361 & 0.059 & 0.294 & 0.307 & 0.285 & 0.308 & 0.105\\\\\\hline\n",
      "963 & 266 & 144 & 697\\\\\\hline\n",
      "oov_count=251\n",
      "0.302 & 0.364 & 0.059 & 0.299 & 0.307 & 0.285 & 0.314 & 0.136\\\\\\hline\n",
      "963 & 270 & 154 & 693\\\\\\hline\n",
      "oov_count=238\n",
      "0.298 & 0.370 & 0.019 & 0.278 & 0.336 & 0.311 & 0.288 & 0.115\\\\\\hline\n",
      "963 & 262 & 155 & 701\\\\\\hline\n",
      "oov_count=238\n",
      "0.298 & 0.369 & 0.019 & 0.277 & 0.337 & 0.312 & 0.288 & 0.121\\\\\\hline\n",
      "963 & 261 & 154 & 702\\\\\\hline\n"
     ]
    }
   ],
   "source": [
    "##No normalization\n",
    "kw_idx_word = create_index(\"../lib/ctms/decode.ctm\", False, False)\n",
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "query_results_sto = search_index(query_list, kw_idx_word)\n",
    "_ = gen_output(query_results_sto, \"../output/decode_sto.xml\")\n",
    "n1m = get_scores(\"decode_sto\",True)\n",
    "kw_idx_word = create_index(\"../lib/ctms/decode.ctm\", True, False)\n",
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "query_results_sto = search_index(query_list, kw_idx_word)\n",
    "_ = gen_output(query_results_sto, \"../output/decode_sto.xml\")\n",
    "n1a = get_scores(\"decode_sto\",True)\n",
    "\n",
    "keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\", False, False)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_sto = search_index(query_list_morph, keyword_index_morph)\n",
    "_ = gen_output(query_results_morph_sto, \"../output/decode_morph_sto.xml\")\n",
    "n2m = get_scores(\"decode_morph_sto\",True)\n",
    "keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\", True, False)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_sto = search_index(query_list_morph, keyword_index_morph)\n",
    "_ = gen_output(query_results_morph_sto, \"../output/decode_morph_sto.xml\")\n",
    "n2a = get_scores(\"decode_morph_sto\",True)\n",
    "\n",
    "keyword_index_morph_manual = create_index(\"../lib/ctms/decode.ctm\", False, True)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_manual_sto = search_index(query_list_morph, keyword_index_morph_manual)\n",
    "_ = gen_output(query_results_morph_manual_sto, \"../output/decode_morph_manual_sto.xml\")\n",
    "n3m = get_scores(\"decode_morph_manual_sto\",True)\n",
    "keyword_index_morph_manual = create_index(\"../lib/ctms/decode.ctm\", True, True)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_manual_sto = search_index(query_list_morph, keyword_index_morph_manual)\n",
    "_ = gen_output(query_results_morph_manual_sto, \"../output/decode_morph_manual_sto.xml\")\n",
    "n3a = get_scores(\"decode_morph_manual_sto\",True)\n",
    "\n",
    "##KST\n",
    "\n",
    "kw_idx_word = create_index(\"../lib/ctms/decode.ctm\", False, False)\n",
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "query_results_sto = search_index(query_list, kw_idx_word)\n",
    "kst_list(query_results_sto)\n",
    "_ = gen_output(query_results_sto, \"../output/decode_sto.xml\")\n",
    "k1m = get_scores(\"decode_sto\",True)\n",
    "kw_idx_word = create_index(\"../lib/ctms/decode.ctm\", True, False)\n",
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "query_results_sto = search_index(query_list, kw_idx_word)\n",
    "kst_list(query_results_sto)\n",
    "_ = gen_output(query_results_sto, \"../output/decode_sto.xml\")\n",
    "k1a = get_scores(\"decode_sto\",True)\n",
    "\n",
    "keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\", False, False)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_sto = search_index(query_list_morph, keyword_index_morph)\n",
    "kst_list(query_results_morph_sto)\n",
    "_ = gen_output(query_results_morph_sto, \"../output/decode_morph_sto.xml\")\n",
    "k2m = get_scores(\"decode_morph_sto\",True)\n",
    "keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\", True, False)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_sto = search_index(query_list_morph, keyword_index_morph)\n",
    "kst_list(query_results_morph_sto)\n",
    "_ = gen_output(query_results_morph_sto, \"../output/decode_morph_sto.xml\")\n",
    "k2a = get_scores(\"decode_morph_sto\",True)\n",
    "\n",
    "keyword_index_morph_manual = create_index(\"../lib/ctms/decode.ctm\", False, True)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_manual_sto = search_index(query_list_morph, keyword_index_morph_manual)\n",
    "kst_list(query_results_morph_manual_sto)\n",
    "_ = gen_output(query_results_morph_manual_sto, \"../output/decode_morph_manual_sto.xml\")\n",
    "k3m = get_scores(\"decode_morph_manual_sto\",True)\n",
    "keyword_index_morph_manual = create_index(\"../lib/ctms/decode.ctm\", True, True)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_manual_sto = search_index(query_list_morph, keyword_index_morph_manual)\n",
    "kst_list(query_results_morph_manual_sto)\n",
    "_ = gen_output(query_results_morph_manual_sto, \"../output/decode_morph_manual_sto.xml\")\n",
    "k3a = get_scores(\"decode_morph_manual_sto\",True)\n",
    "\n",
    "##STO\n",
    "\n",
    "kw_idx_word = create_index(\"../lib/ctms/decode.ctm\", False, False)\n",
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "query_results_sto = search_index(query_list, kw_idx_word)\n",
    "normalize_list(query_results_sto,1)\n",
    "_ = gen_output(query_results_sto, \"../output/decode_sto.xml\")\n",
    "s1m = get_scores(\"decode_sto\",True)\n",
    "kw_idx_word = create_index(\"../lib/ctms/decode.ctm\", True, False)\n",
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "query_results_sto = search_index(query_list, kw_idx_word)\n",
    "normalize_list(query_results_sto,1)\n",
    "_ = gen_output(query_results_sto, \"../output/decode_sto.xml\")\n",
    "s1a = get_scores(\"decode_sto\",True)\n",
    "\n",
    "keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\", False, False)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_sto = search_index(query_list_morph, keyword_index_morph)\n",
    "normalize_list(query_results_morph_sto,1)\n",
    "_ = gen_output(query_results_morph_sto, \"../output/decode_morph_sto.xml\")\n",
    "s2m = get_scores(\"decode_morph_sto\",True)\n",
    "keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\", True, False)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_sto = search_index(query_list_morph, keyword_index_morph)\n",
    "normalize_list(query_results_morph_sto,1)\n",
    "_ = gen_output(query_results_morph_sto, \"../output/decode_morph_sto.xml\")\n",
    "s2a = get_scores(\"decode_morph_sto\",True)\n",
    "\n",
    "keyword_index_morph_manual = create_index(\"../lib/ctms/decode.ctm\", False, True)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_manual_sto = search_index(query_list_morph, keyword_index_morph_manual)\n",
    "normalize_list(query_results_morph_manual_sto,1)\n",
    "_ = gen_output(query_results_morph_manual_sto, \"../output/decode_morph_manual_sto.xml\")\n",
    "s3m = get_scores(\"decode_morph_manual_sto\",True)\n",
    "keyword_index_morph_manual = create_index(\"../lib/ctms/decode.ctm\", True, True)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_manual_sto = search_index(query_list_morph, keyword_index_morph_manual)\n",
    "normalize_list(query_results_morph_manual_sto,1)\n",
    "_ = gen_output(query_results_morph_manual_sto, \"../output/decode_morph_manual_sto.xml\")\n",
    "s3a = get_scores(\"decode_morph_manual_sto\",True)\n",
    "\n",
    "##KST + STO\n",
    "\n",
    "kw_idx_word = create_index(\"../lib/ctms/decode.ctm\", False, False)\n",
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "query_results_sto = search_index(query_list, kw_idx_word)\n",
    "kst_list(query_results_sto)\n",
    "normalize_list(query_results_sto,1)\n",
    "_ = gen_output(query_results_sto, \"../output/decode_sto.xml\")\n",
    "ks1m = get_scores(\"decode_sto\",True)\n",
    "kw_idx_word = create_index(\"../lib/ctms/decode.ctm\", True, False)\n",
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "query_results_sto = search_index(query_list, kw_idx_word)\n",
    "kst_list(query_results_sto)\n",
    "normalize_list(query_results_sto,1)\n",
    "_ = gen_output(query_results_sto, \"../output/decode_sto.xml\")\n",
    "ks1a = get_scores(\"decode_sto\",True)\n",
    "\n",
    "keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\", False, False)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_sto = search_index(query_list_morph, keyword_index_morph)\n",
    "kst_list(query_results_morph_sto)\n",
    "normalize_list(query_results_morph_sto,1)\n",
    "_ = gen_output(query_results_morph_sto, \"../output/decode_morph_sto.xml\")\n",
    "ks2m = get_scores(\"decode_morph_sto\",True)\n",
    "keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\", True, False)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_sto = search_index(query_list_morph, keyword_index_morph)\n",
    "kst_list(query_results_morph_sto)\n",
    "normalize_list(query_results_morph_sto,1)\n",
    "_ = gen_output(query_results_morph_sto, \"../output/decode_morph_sto.xml\")\n",
    "ks2a = get_scores(\"decode_morph_sto\",True)\n",
    "\n",
    "keyword_index_morph_manual = create_index(\"../lib/ctms/decode.ctm\", False, True)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_manual_sto = search_index(query_list_morph, keyword_index_morph_manual)\n",
    "kst_list(query_results_morph_manual_sto)\n",
    "normalize_list(query_results_morph_manual_sto,1)\n",
    "_ = gen_output(query_results_morph_manual_sto, \"../output/decode_morph_manual_sto.xml\")\n",
    "ks3m = get_scores(\"decode_morph_manual_sto\",True)\n",
    "keyword_index_morph_manual = create_index(\"../lib/ctms/decode.ctm\", True, True)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_manual_sto = search_index(query_list_morph, keyword_index_morph_manual)\n",
    "kst_list(query_results_morph_manual_sto)\n",
    "normalize_list(query_results_morph_manual_sto,1)\n",
    "_ = gen_output(query_results_morph_manual_sto, \"../output/decode_morph_manual_sto.xml\")\n",
    "ks3a = get_scores(\"decode_morph_manual_sto\",True)\n",
    "\n",
    "## STO + KST\n",
    "\n",
    "kw_idx_word = create_index(\"../lib/ctms/decode.ctm\", False, False)\n",
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "query_results_sto = search_index(query_list, kw_idx_word)\n",
    "normalize_list(query_results_sto,1)\n",
    "kst_list(query_results_sto)\n",
    "_ = gen_output(query_results_sto, \"../output/decode_sto.xml\")\n",
    "sk1m = get_scores(\"decode_sto\",True)\n",
    "kw_idx_word = create_index(\"../lib/ctms/decode.ctm\", True, False)\n",
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "query_results_sto = search_index(query_list, kw_idx_word)\n",
    "normalize_list(query_results_sto,1)\n",
    "kst_list(query_results_sto)\n",
    "_ = gen_output(query_results_sto, \"../output/decode_sto.xml\")\n",
    "sk1a = get_scores(\"decode_sto\",True)\n",
    "\n",
    "keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\", False, False)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_sto = search_index(query_list_morph, keyword_index_morph)\n",
    "normalize_list(query_results_morph_sto,1)\n",
    "kst_list(query_results_morph_sto)\n",
    "_ = gen_output(query_results_morph_sto, \"../output/decode_morph_sto.xml\")\n",
    "sk2m = get_scores(\"decode_morph_sto\",True)\n",
    "keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\", True, False)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_sto = search_index(query_list_morph, keyword_index_morph)\n",
    "normalize_list(query_results_morph_sto,1)\n",
    "kst_list(query_results_morph_sto)\n",
    "_ = gen_output(query_results_morph_sto, \"../output/decode_morph_sto.xml\")\n",
    "sk2a = get_scores(\"decode_morph_sto\",True)\n",
    "\n",
    "keyword_index_morph_manual = create_index(\"../lib/ctms/decode.ctm\", False, True)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_manual_sto = search_index(query_list_morph, keyword_index_morph_manual)\n",
    "normalize_list(query_results_morph_manual_sto,1)\n",
    "kst_list(query_results_morph_manual_sto)\n",
    "_ = gen_output(query_results_morph_manual_sto, \"../output/decode_morph_manual_sto.xml\")\n",
    "sk3m = get_scores(\"decode_morph_manual_sto\",True)\n",
    "keyword_index_morph_manual = create_index(\"../lib/ctms/decode.ctm\", True, True)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_manual_sto = search_index(query_list_morph, keyword_index_morph_manual)\n",
    "normalize_list(query_results_morph_manual_sto,1)\n",
    "kst_list(query_results_morph_manual_sto)\n",
    "_ = gen_output(query_results_morph_manual_sto, \"../output/decode_morph_manual_sto.xml\")\n",
    "sk3a = get_scores(\"decode_morph_manual_sto\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word & Average & 0.320 & 0.320 & 0.320 & 0.320 & 0.295& 0.295\\\\\\hline\n",
      "Word & Multiply & 0.319 & 0.320 & 0.320 & 0.320 & 0.296& 0.296\\\\\\hline\n",
      "Morph-Based ASR & Average & 0.318 & 0.320 & 0.326 & 0.326 & 0.302& 0.302\\\\\\hline\n",
      "Morph-Based ASR & Multiply & 0.318 & 0.321 & 0.326 & 0.326 & 0.299& 0.299\\\\\\hline\n",
      "ASR + Morph-Dict & Average & 0.316 & 0.317 & 0.320 & 0.320 & 0.298& 0.298\\\\\\hline\n",
      "ASR + Morph-Dict & Multiply & 0.314 & 0.317 & 0.319 & 0.318 & 0.298& 0.298\\\\\\hline\n"
     ]
    }
   ],
   "source": [
    "print \"Word & Average & {:1.3f} & {:1.3f} & {:1.3f} & {:1.3f} & {:1.3f}& {:1.3f}\\\\\\\\\\hline\".format(n1a['all'][0],k1a['all'][0],s1a['all'][0],ks1a['all'][0],sk1a['all'][0])\n",
    "print \"Word & Multiply & {:1.3f} & {:1.3f} & {:1.3f} & {:1.3f} & {:1.3f}& {:1.3f}\\\\\\\\\\hline\".format(n1m['all'][0],k1m['all'][0],s1m['all'][0],ks1m['all'][0],sk1m['all'][0])\n",
    "print \"Morph-Based ASR & Average & {:1.3f} & {:1.3f} & {:1.3f} & {:1.3f} & {:1.3f}& {:1.3f}\\\\\\\\\\hline\".format(n2a['all'][0],k2a['all'][0],s2a['all'][0],ks2a['all'][0],sk2a['all'][0])\n",
    "print \"Morph-Based ASR & Multiply & {:1.3f} & {:1.3f} & {:1.3f} & {:1.3f} & {:1.3f}& {:1.3f}\\\\\\\\\\hline\".format(n2m['all'][0],k2m['all'][0],s2m['all'][0],ks2m['all'][0],sk2m['all'][0])\n",
    "print \"ASR + Morph-Dict & Average & {:1.3f} & {:1.3f} & {:1.3f} & {:1.3f} & {:1.3f}& {:1.3f}\\\\\\\\\\hline\".format(n3a['all'][0],k3a['all'][0],s3a['all'][0],ks3a['all'][0],sk3a['all'][0])\n",
    "print \"ASR + Morph-Dict & Multiply & {:1.3f} & {:1.3f} & {:1.3f} & {:1.3f} & {:1.3f}& {:1.3f}\\\\\\\\\\hline\".format(n3m['all'][0],k3m['all'][0],s3m['all'][0],ks3m['all'][0],sk3m['all'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! head ../lib/kws/grapheme.map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Grapheme Dict and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "class grapheme_dict:\n",
    "    def __init__(self):\n",
    "        self.dict = [[0 for x in range(27)] for y in range(27)] \n",
    "        self.total = [0 for x in range(27)]\n",
    "        \n",
    "    def update(self,hyp,ref,score_str):\n",
    "        h = self.get_index(hyp)\n",
    "        r = self.get_index(ref)\n",
    "        score = (float(score_str))\n",
    "        self.dict[h][r] = score \n",
    "        \n",
    "    def normalize(self):\n",
    "        self.total = [0 for x in range(27)]\n",
    "        for i in range(27):\n",
    "            for j in range(27):\n",
    "                self.total[i] = self.total[i] + self.dict[i][j]\n",
    "        for i in range(27):\n",
    "            for j in range(27):\n",
    "                val = self.dict[i][j]/self.total[i]\n",
    "                if (val!=0):\n",
    "                     self.dict[i][j] = -log(val)\n",
    "                \n",
    "    def get_dist(self,hyp,ref,oov_first=True):\n",
    "        if hyp==' ':\n",
    "            hyp='sil'\n",
    "        if ref==' ':\n",
    "            ref='sil'\n",
    "        if(oov_first):\n",
    "            h = self.get_index(hyp)\n",
    "            r = self.get_index(ref)\n",
    "        else:\n",
    "            r = self.get_index(hyp)\n",
    "            h = self.get_index(ref)\n",
    "        \n",
    "        return self.dict[h][r]\n",
    "                \n",
    "    def create_fst(self):\n",
    "        fst_compiler_list = []\n",
    "        for i in range(27):\n",
    "            for j in range(27):\n",
    "                if (self.dict[i][j] != 0):\n",
    "                    c_i = self.get_char(i)\n",
    "                    c_j = self.get_char(j)\n",
    "                    if c_i == 'sil':\n",
    "                        c_i = '<eps>'\n",
    "                    if c_j == 'sil':\n",
    "                        c_j = '<eps>'\n",
    "                    fst_compiler_list.append(\"{} {} {} {} {}\".format(0,0,c_i,c_j,(self.dict[i][j])))\n",
    "        fst_compiler_list.append(str(0))\n",
    "        return fst_compiler_list\n",
    "    \n",
    "    def symbol_list(self):\n",
    "        symbol_list = []\n",
    "        for i in range(27):\n",
    "            symbol_list.append(self.get_char(i))\n",
    "        return symbol_list\n",
    "    \n",
    "    def get_char(self, x):\n",
    "        if x==0:\n",
    "            return 'sil'\n",
    "        else:\n",
    "            return chr(x+96)\n",
    "        \n",
    "    def get_index(self, x):\n",
    "        if x=='sil':\n",
    "            return 0\n",
    "        else:\n",
    "            return ord(x)-96  \n",
    "    \n",
    "    def print_file(self,filename):\n",
    "        arr = list(\"abcdefghijklmnopqrstuvwxyz \")\n",
    "        with open(filename, 'w') as myfile:\n",
    "            for ch1 in arr:\n",
    "                for ch2 in arr:\n",
    "                    myfile.write(\"{} {} {}\\n\".format('{' if ch1 == \" \" else ch1, '{' if ch2 == \" \" else ch2, self.get_dist(ch1,ch2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../lib/kws/grapheme.map\", \"r\") as myfile:\n",
    "    data=myfile.readlines()\n",
    "\n",
    "g_dict = grapheme_dict()\n",
    "for line in data:\n",
    "    ref, hyp, score_str = line.split()\n",
    "    g_dict.update(hyp,ref,score_str)\n",
    "g_dict.normalize()\n",
    "g_dict.print_file(\"./C/gmap.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find IV replacements for OOV words using edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_replacements_final(oov_word_unfiltered, iv_list_unfiltered, g_dict, threshold, allowed_len_diff):\n",
    "    #threshold=15\n",
    "    #allowed_len_diff=5\n",
    "    oov_word = \"\".join([x for x in oov_word_unfiltered if str.isalpha(x) or x==' '])\n",
    "    iv_list = [x for x in iv_list_unfiltered if abs(len(x)-len(oov_word_unfiltered))<=allowed_len_diff]\n",
    "    min_dist_word=\"\"\n",
    "    min_dist_score=1000\n",
    "\n",
    "    for hyp_word_unfiltered in iv_list:\n",
    "        hyp_word = \"\".join([x for x in hyp_word_unfiltered if str.isalpha(x) or x==' '])\n",
    "        space_score=[]\n",
    "        #spaces = []\n",
    "        #scores = []\n",
    "        if (len(hyp_word)>len(oov_word)):\n",
    "            maxlen=len(hyp_word)\n",
    "            minlen=len(oov_word)\n",
    "            pad_word=oov_word\n",
    "            org_word=hyp_word\n",
    "            iv_first=True;\n",
    "        else:\n",
    "            maxlen=len(oov_word)\n",
    "            minlen=len(hyp_word)\n",
    "            pad_word=hyp_word\n",
    "            org_word=oov_word\n",
    "            iv_first=False;\n",
    "            #(org_word_char, pad_word_char, iv_first)\n",
    "\n",
    "        len_diff = maxlen-minlen\n",
    "        space_score.append((len_diff,0))\n",
    "        #repl_counter=0\n",
    "        #print len_diff\n",
    "        \n",
    "        search = True\n",
    "        char_idx=0\n",
    "        while(search):\n",
    "        #for char_idx in range(0,maxlen):\n",
    "            #print char_idx, len(space_score)\n",
    "            min_score=[1000 for x in range(0,30)]\n",
    "            new_space_score = []\n",
    "            \n",
    "            #print \"=== {}\".format(char_idx)\n",
    "            #print space_score\n",
    "\n",
    "            for repl_word in space_score:\n",
    "                #substitution\n",
    "                pad_word_pos = char_idx - (len_diff - repl_word[0])\n",
    "                if(pad_word_pos<minlen):\n",
    "                    #print pad_word_pos\n",
    "                    new_score = repl_word[1]+g_dict.get_dist(pad_word[pad_word_pos],org_word[char_idx],iv_first)\n",
    "                    new_space_score.append((repl_word[0],new_score))\n",
    "                    #print (org_word[char_idx],pad_word[pad_word_pos],new_score)\n",
    "                    if min_score[repl_word[0]]>new_score:\n",
    "                        min_score[repl_word[0]]=new_score\n",
    "\n",
    "\n",
    "                #padding\n",
    "                if(repl_word[0]>0):\n",
    "                    new_score = repl_word[1]+g_dict.get_dist(' ',org_word[char_idx],iv_first)\n",
    "                    new_space_score.append((repl_word[0]-1,new_score))\n",
    "                    if min_score[repl_word[0]-1]>new_score:\n",
    "                        min_score[repl_word[0]-1]=new_score\n",
    "            #print new_space_score\n",
    "            space_score=[]\n",
    "            \n",
    "            #print \"==========={}\".format(len(new_space_score))\n",
    "            for repl_word in new_space_score:\n",
    "                if (repl_word[1]<=min(min_score[repl_word[0]], min_dist_score, threshold)):\n",
    "                    space_score.append(repl_word)\n",
    "            char_idx+=1\n",
    "            \n",
    "            if (char_idx==maxlen or len(space_score)==0):\n",
    "                search=False\n",
    "            #print \"==========={}\".format(len(space_score))\n",
    "            \n",
    "        #extract lowest distance word\n",
    "        min_word_sc=1000\n",
    "        for repl_word in space_score:\n",
    "            if repl_word[1]<min_word_sc:\n",
    "                min_word_sc=repl_word[1]\n",
    "        #print (hyp_word_unfiltered, pad_word, org_word, min_word_sc)\n",
    "        if min_word_sc<min_dist_score:\n",
    "            min_dist_score=min_word_sc\n",
    "            min_dist_word=hyp_word_unfiltered\n",
    "    return min_dist_word, min_dist_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updated Index Searching Functions for OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def save_oov_list(oov_list_unfiltered, filename):\n",
    "    maxlen=28\n",
    "    oov_list = [x for x in oov_list_unfiltered if len(x)<maxlen]\n",
    "    with open(filename, 'w') as myfile:\n",
    "        myfile.write(\"{} {}\\n\".format(maxlen+2, len(oov_list)))\n",
    "        for word in oov_list:\n",
    "            word_filtered = \"\".join([ch for ch in word if (str.isalpha(ch) or ch==' ')])\n",
    "            myfile.write(\"{}\\n\".format(word_filtered.replace(\" \",\"{\")))\n",
    "    return\n",
    "\n",
    "def read_results(filename):\n",
    "    with open(filename, \"r\") as data:\n",
    "        lines = data.readlines()\n",
    "    return lines;\n",
    "\n",
    "def save_iv_list(kw_idx, filename):\n",
    "    maxlen=28\n",
    "    iv_list = [x for x in kw_idx.keys() if len(x)<maxlen]\n",
    "    with open(filename, 'w') as myfile:\n",
    "        myfile.write(\"{} {}\\n\".format(maxlen+2, len(iv_list)))\n",
    "        for word in iv_list:\n",
    "            word_filtered = \"\".join([ch for ch in word if (str.isalpha(ch) or ch==' ')])\n",
    "            myfile.write(\"{}\\n\".format(word_filtered.replace(\" \",\"{\")))\n",
    "    return iv_list\n",
    "\n",
    "def search_index_oov(search_list, indexed_dict, grapheme_dict, threshold, allowed_len_diff, gamma=0): \n",
    "    maxlen=28\n",
    "    iv_list = [x for x in indexed_dict.keys() if len(x)<maxlen]\n",
    "    total_search_time = 0\n",
    "    oov_list = []\n",
    "    for q in search_list:\n",
    "        oov_count = 0\n",
    "        if q.text in indexed_dict:\n",
    "            q.res_list = indexed_dict[q.text]\n",
    "        else:\n",
    "            #oov_list.append(q.text)\n",
    "            start_t = timer()\n",
    "            iv_term, iv_score = find_replacements_final(q.text, iv_list, grapheme_dict, threshold, allowed_len_diff)\n",
    "            end_t = timer()\n",
    "            print (q.text, iv_term, iv_score, end_t-start_t)\n",
    "            total_search_time += end_t-start_t\n",
    "            if iv_score<=threshold:\n",
    "                q.res_list=indexed_dict[iv_term]\n",
    "        q.oov = oov_count\n",
    "        if (gamma>0):\n",
    "            q.normalize(gamma)\n",
    "        #q.search_time = end - start\n",
    "    return search_list#, oov_list\n",
    "\n",
    "\n",
    "def search_index_oov_c(search_list, indexed_dict, grapheme_dict, threshold, allowed_len_diff, gamma=0): \n",
    "    total_search_time = 0\n",
    "    oov_text_list = []\n",
    "    oov_idx_list = []\n",
    "    start = timer()\n",
    "    oov_count = 0\n",
    "    for i,q in enumerate(search_list):\n",
    "        if q.text in indexed_dict:\n",
    "            q.res_list = indexed_dict[q.text]\n",
    "        else:\n",
    "            oov_text_list.append(q.text)\n",
    "            oov_idx_list.append(i)\n",
    "            oov_count +=1\n",
    "        q.oov = 1\n",
    "    print \"oov_count={}\".format(oov_count)\n",
    "    iv_list = save_iv_list(indexed_dict, \"./C/iv_list.txt\")\n",
    "    subprocess.call([\"./C/oov_search\",str(allowed_len_diff), str(threshold)])\n",
    "    oov_results = parallel_oov_replacements(8, oov_text_list, threshold, allowed_len_diff)\n",
    "    #save_oov_list(oov_text_list, \"./C/oov_list.txt\")\n",
    "    #subprocess.call([\"./C/oov_search\",str(allowed_len_diff), str(threshold)])\n",
    "    #oov_results = read_results(\"./C/oov_hyps.txt\")\n",
    "    for i,oov_res in enumerate(oov_results):\n",
    "        #print (i,oov_res)\n",
    "        idx=int(oov_res.split()[0])\n",
    "        if (idx)>0:\n",
    "            q=search_list[oov_idx_list[i]]\n",
    "            iv_list_idx=int(oov_res.split()[0])\n",
    "            #print (idx,iv_list_idx)\n",
    "            q.res_list=indexed_dict[iv_list[iv_list_idx]]\n",
    "            #print(q.text,iv_list[iv_list_idx])\n",
    "    end = timer()\n",
    "    #print (end - start)\n",
    "    return search_list\n",
    "\n",
    "def parallel_oov_replacements(num_proc, oov_list, threshold, allowed_len_diff):\n",
    "    list_len = len(oov_list)\n",
    "    limits=[]\n",
    "    #save_iv_list(keyword_index, \"./C/iv_list.txt\")\n",
    "    for i in range(0,list_len+1,list_len/num_proc):\n",
    "        limits.append(i)\n",
    "    limits[-1]=list_len\n",
    "    #print limits\n",
    "    oov_files=[]\n",
    "    op_files=[]\n",
    "    processes=set()\n",
    "    for i in range(0,num_proc):\n",
    "        save_oov_list(oov_list[limits[i]:limits[i+1]],\"./C/oov_list_{}.txt\".format(i))\n",
    "        #print \"./C/oov_list_{}.txt\".format(i)\n",
    "        #print (oov_list[limits[i]:limits[i+1]])\n",
    "        oov_files.append(\"oov_list_{}.txt\".format(i))\n",
    "        op_files.append(\"oov_hyps_{}.txt\".format(i))\n",
    "    for i in range(0,num_proc):\n",
    "        #print \" \".join([\"./oov_search\",str(allowed_len_diff), str(threshold), oov_files[i], op_files[i]])\n",
    "        processes.add(subprocess.Popen([\"./oov_search\",str(allowed_len_diff), str(threshold), oov_files[i], op_files[i]], cwd=\"./C\"))\n",
    "    for p in processes:\n",
    "        if p.poll() is None:\n",
    "            p.wait()\n",
    "    oov_results=[]\n",
    "    for i in range(0,num_proc):\n",
    "        oov_results = oov_results + read_results(\"./C/{}\".format(op_files[i]))\n",
    "    subprocess.call([\"rm\",\"iv_list.txt\"] + oov_files + op_files, cwd=\"./C\")\n",
    "    return oov_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word-Based Decode with OOV Replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oov_count=246\n",
      "all (488) - TWV:0.3735\n",
      "iv (388) - TWV:0.4411\n",
      "oov (100) - TWV:0.1112\n",
      "short (321) - TWV:0.3324\n",
      "long (167) - TWV:0.4526\n",
      "word (288) - TWV:0.3614\n",
      "phrase (200) - TWV:0.3909\n",
      "Threshold: 0.167\n",
      "Targets: 963 Correct: 451 False Alarms: 837 Miss: 512\n",
      "Time Taken: 4.10971617699\n"
     ]
    }
   ],
   "source": [
    "keyword_index = create_index(\"../lib/ctms/decode.ctm\", True, False)\n",
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "\n",
    "start = timer()\n",
    "query_results_oov_c = search_index_oov_c(query_list, keyword_index, g_dict, 30, 7)\n",
    "#kst_list(query_results_oov_c)\n",
    "#normalize_list(query_results_oov_c,1)\n",
    "end = timer()\n",
    "\n",
    "_ = gen_output(query_results_oov_c, \"../output/decode_oov.xml\")\n",
    "_ = get_scores(\"decode_oov\")\n",
    "print (\"Time Taken: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-decomposed Morphemes with OOV Replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oov_count=251\n",
      "all (488) - TWV:0.3763\n",
      "iv (388) - TWV:0.4327\n",
      "oov (100) - TWV:0.1574\n",
      "short (321) - TWV:0.3376\n",
      "long (167) - TWV:0.4507\n",
      "word (288) - TWV:0.3601\n",
      "phrase (200) - TWV:0.3995\n",
      "Threshold: 0.055\n",
      "Targets: 963 Correct: 446 False Alarms: 1273 Miss: 517\n",
      "Time Taken: 13.5082330704\n"
     ]
    }
   ],
   "source": [
    "keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\", True, False)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\",True)\n",
    "\n",
    "start = timer()\n",
    "query_results_oov_morph_c = search_index_oov_c(query_list_morph, keyword_index_morph, g_dict, 30, 5)\n",
    "kst_list(query_results_oov_morph_c)\n",
    "normalize_list(query_results_oov_morph_c,1)\n",
    "end = timer()\n",
    "\n",
    "_ = gen_output(query_results_oov_morph_c, \"../output/decode_morph_oov.xml\")\n",
    "_ = get_scores(\"decode_morph_oov\")\n",
    "print (\"Time Taken: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Decomposed Morphemes with OOV Replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oov_count=238\n",
      "all (488) - TWV:0.3571\n",
      "iv (388) - TWV:0.4164\n",
      "oov (100) - TWV:0.1270\n",
      "short (321) - TWV:0.3069\n",
      "long (167) - TWV:0.4537\n",
      "word (288) - TWV:0.3427\n",
      "phrase (200) - TWV:0.3778\n",
      "Threshold: 0.205\n",
      "Targets: 963 Correct: 457 False Alarms: 1194 Miss: 506\n",
      "Time Taken: 12.2436859608\n"
     ]
    }
   ],
   "source": [
    "kw_idx_morph_manual = create_index(\"../lib/ctms/decode.ctm\", True, True)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "\n",
    "start = timer()\n",
    "query_results_morph_manual_oov = search_index_oov_c(query_list_morph, kw_idx_morph_manual, g_dict, 30, 5)\n",
    "#kst_list(query_results_morph_manual_oov)\n",
    "#normalize_list(query_results_morph_manual_oov,1)\n",
    "end = timer()\n",
    "\n",
    "_ = gen_output(query_results_morph_manual_oov, \"../output/decode_morph_manual_oov.xml\")\n",
    "_ = get_scores(\"decode_morph_manual_oov\")\n",
    "print (\"Time Taken: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.379 & 0.447 & 0.116 & 0.341 & 0.453 & 0.393 & 0.369 & 0.045\\\\\\hline\n",
      "963 & 444 & 733 & 519\\\\\\hline\n",
      "0.374 & 0.435 & 0.137 & 0.327 & 0.463 & 0.399 & 0.357 & 0.064\\\\\\hline\n",
      "963 & 443 & 890 & 520\\\\\\hline\n",
      "0.376 & 0.433 & 0.157 & 0.338 & 0.451 & 0.400 & 0.360 & 0.055\\\\\\hline\n",
      "963 & 446 & 1273 & 517\\\\\\hline\n"
     ]
    }
   ],
   "source": [
    "_ = get_scores(\"decode_oov\",True)\n",
    "_ = get_scores(\"decode_morph_oov\", True)\n",
    "_ = get_scores(\"decode_morph_manual_oov\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Experiment: System Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_index = create_index(\"../lib/ctms/decode.ctm\")\n",
    "keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "def read_posting_list(filepath):\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    search_dict = {}\n",
    "    search_list = []\n",
    "    for child in root:\n",
    "        #print child.tag, child.attrib\n",
    "        kwid = child.attrib['kwid']\n",
    "        new_res = query(kwid, \"unk\", \"unk\")\n",
    "        for res in child:\n",
    "            score = float(res.attrib['score'])\n",
    "            dur = float(res.attrib['dur'])\n",
    "            filename = res.attrib['file']\n",
    "            channel = int(res.attrib['channel'])\n",
    "            start_time = float(res.attrib['tbeg'])\n",
    "            new_res.res_list.append([filename, channel, start_time, dur, score])\n",
    "        search_dict[kwid]=new_res\n",
    "        #search_list.append(new_res)\n",
    "    with open(\"../lib/kws/kwlist\") as myfile:\n",
    "        kwlist = myfile.read().splitlines()\n",
    "    for kw in kwlist:\n",
    "        if kw in search_dict.keys():\n",
    "            search_list.append(search_dict[kw])\n",
    "        else:\n",
    "            search_list.append(query(kw, \"unk\", \"unk\"))\n",
    "    return search_list\n",
    "\n",
    "def sys_comb(query_results_q1, query_results_q2, w1=0.5, w2=0.5, use_max=False):\n",
    "    if (use_max):\n",
    "        w1=1.0\n",
    "        w2=1.0\n",
    "    search_list = []\n",
    "    for q1, q2 in zip(query_results_q1, query_results_q2):\n",
    "        c2 = query(q1.kwid, q1.text)\n",
    "        #print q1.kwid, q2.kwid\n",
    "        q1_res_list = [] \n",
    "        for hit in q1.res_list:\n",
    "            q1_res_list.append(hit + [False])\n",
    "\n",
    "        q2_res_list = []\n",
    "        for hit in q2.res_list:\n",
    "            q2_res_list.append(hit+[False])\n",
    "        #print (q1.text, q2.text, len(q1.res_list),len(q2.res_list))\n",
    "        new_res_list=[]\n",
    "\n",
    "        #file, int(channel), float(startTime), float(duration), float(score)\n",
    "\n",
    "        for q1_hit in q1_res_list:\n",
    "            q1_start_time = q1_hit[2]\n",
    "            q1_dur = q1_hit[3]\n",
    "            q1_score = q1_hit[4]# * w1#Scoring Averages WCombSum - W1\n",
    "\n",
    "            for q2_hit in q2_res_list:\n",
    "                q2_start_time = q2_hit[2]\n",
    "                q2_dur = q2_hit[3]\n",
    "                q2_score = q2_hit[4]# * w2#Scoring W2\n",
    "\n",
    "                #if abs(q1_start_time - q2_start_time) < 0.2:#Merge\n",
    "                if (q1_start_time<=q2_start_time and q1_start_time+q1_dur>=q2_start_time) or (q2_start_time<=q1_start_time and q2_start_time+q2_dur>=q1_start_time):\n",
    "                    #print q1_res_list\n",
    "                    q1_hit[5]=True\n",
    "                    q2_hit[5]=True\n",
    "                    #print q1_res_list\n",
    "                    new_score = q1_score * w1 + q2_score * w2\n",
    "                    if(use_max):\n",
    "                        new_score = max(q1_score,q2_score)\n",
    "                    if q1_score>q2_score:\n",
    "                        new_start_time = q1_start_time\n",
    "                        new_duration = q1_dur\n",
    "                    else:\n",
    "                        new_start_time = q2_start_time\n",
    "                        new_duration = q2_dur\n",
    "                    #new_start_time = min(q1_start_time, q2_start_time)\n",
    "                    #if q1_start_time<q2_start_time\n",
    "                    #new_duration = max(q1_dur,q2_dur)\n",
    "                    new_res = q1_hit[0:2] + [new_start_time, new_duration, new_score]\n",
    "                    new_res_list.append(new_res)\n",
    "            if(q1_hit[5]==False):\n",
    "                new_res = q1_hit[0:4] + [q1_score * w1]\n",
    "                new_res_list.append(new_res)\n",
    "\n",
    "        for q2_hit in q2_res_list:\n",
    "            if(q2_hit[5]==False):\n",
    "                q2_score = q2_hit[4]# * w2\n",
    "                new_res = q2_hit[0:4] + [q2_score *w2]\n",
    "                new_res_list.append(new_res)\n",
    "\n",
    "        c2.res_list = new_res_list\n",
    "        search_list.append(c2)\n",
    "    return search_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.520 & 0.559 & 0.367 & 0.496 & 0.565 & 0.512 & 0.526 & 0.039\\\\\\hline\n",
      "963 & 691 & 9463 & 272\\\\\\hline\n",
      "0.460 & 0.579 & 0.000 & 0.439 & 0.501 & 0.484 & 0.444 & 0.036\\\\\\hline\n",
      "963 & 715 & 13817 & 248\\\\\\hline\n",
      "0.465 & 0.585 & 0.000 & 0.450 & 0.495 & 0.487 & 0.450 & 0.030\\\\\\hline\n",
      "963 & 698 & 12649 & 265\\\\\\hline\n"
     ]
    }
   ],
   "source": [
    "morph_best_list = read_posting_list(\"../lib/kws/morph.xml\")\n",
    "normalize_list(morph_best_list,1)\n",
    "_ = gen_output(morph_best_list, \"../output/ref_morph.xml\")\n",
    "_ = get_scores(\"ref_morph\", True)\n",
    "word_best_list = read_posting_list(\"../lib/kws/word.xml\")\n",
    "normalize_list(word_best_list,1)\n",
    "_ = gen_output(word_best_list, \"../output/ref_word.xml\")\n",
    "_ = get_scores(\"ref_word\", True)\n",
    "word_sys2_best_list = read_posting_list(\"../lib/kws/word-sys2.xml\")\n",
    "normalize_list(word_sys2_best_list,1)\n",
    "_ = gen_output(word_sys2_best_list, \"../output/ref_word-sys2.xml\")\n",
    "_ = get_scores(\"ref_word-sys2\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oov_count=246\n",
      "oov_count=251\n",
      "all (488) - TWV:0.3616\n",
      "iv (388) - TWV:0.4372\n",
      "oov (100) - TWV:0.0680\n",
      "short (321) - TWV:0.3602\n",
      "long (167) - TWV:0.3643\n",
      "word (288) - TWV:0.3721\n",
      "phrase (200) - TWV:0.3464\n",
      "Threshold: 0.038\n",
      "Targets: 963 Correct: 450 False Alarms: 708 Miss: 513\n"
     ]
    }
   ],
   "source": [
    "kw_idx_word = create_index(\"../lib/ctms/decode.ctm\", True, False)\n",
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "query_results_sto = search_index(query_list, kw_idx_word)\n",
    "normalize_list(query_results_sto,1)\n",
    "\n",
    "keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\", True, False)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_sto = search_index(query_list_morph, keyword_index_morph)\n",
    "normalize_list(query_results_morph_sto, 1)\n",
    "\n",
    "word_morph_list = sys_comb(query_results_morph_sto, query_results_sto, 0.5,0.5)\n",
    "\n",
    "_ = gen_output(word_morph_list, \"../output/decode_morph_sto.xml\")\n",
    "_ = get_scores(\"decode_morph_sto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining results from diverse ASR systems, we show good robustness across\n",
    "a wide variety of talkers, channels, environments, and target terms.\n",
    "Second, we compare score normalization approaches for STD. The\n",
    "score normalization is relevant to data fusion since those scores provided\n",
    "by the different systems are not comparable. Therefore, score\n",
    "normalization is often performed as a preliminary step to data fusion.\n",
    "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639278&tag=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all (488) - TWV:0.4969\n",
      "iv (388) - TWV:0.5315\n",
      "oov (100) - TWV:0.3627\n",
      "short (321) - TWV:0.4526\n",
      "long (167) - TWV:0.5820\n",
      "word (288) - TWV:0.4782\n",
      "phrase (200) - TWV:0.5238\n",
      "Threshold: 0.031\n",
      "Targets: 963 Correct: 761 False Alarms: 22148 Miss: 202\n"
     ]
    }
   ],
   "source": [
    "morph_twv = 0.359\n",
    "word_twv=0.398\n",
    "sys2_twv=0.403\n",
    "\n",
    "word_morph_list = sys_comb(morph_best_list, word_best_list, 0.5, 0.5)#morph_twv/(morph_twv+word_twv), word_twv/(morph_twv+word_twv)\n",
    "_ = gen_output(word_morph_list, \"../output/ref_word_morph.xml\")\n",
    "_ = get_scores(\"ref_word_morph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all (488) - TWV:0.4828\n",
      "iv (388) - TWV:0.5127\n",
      "oov (100) - TWV:0.3669\n",
      "short (321) - TWV:0.4434\n",
      "long (167) - TWV:0.5585\n",
      "word (288) - TWV:0.4661\n",
      "phrase (200) - TWV:0.5068\n",
      "Threshold: 0.056\n",
      "Targets: 963 Correct: 779 False Alarms: 30364 Miss: 184\n"
     ]
    }
   ],
   "source": [
    "word_morph_sys2 = sys_comb(word_morph_list, word_sys2_best_list, 1, 0.5)\n",
    "_ = gen_output(word_morph_sys2, \"../output/ref_word_morph_sys2.xml\")\n",
    "_ = get_scores(\"ref_word_morph_sys2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.330 & 0.415 & 0.000 & 0.302 & 0.383 & 0.371 & 0.301 & 0.045\\\\\\hline\n",
      "963 & 368 & 101 & 595\\\\\\hline\n",
      "0.335 & 0.394 & 0.108 & 0.313 & 0.378 & 0.353 & 0.323 & 0.067\\\\\\hline\n",
      "963 & 378 & 274 & 585\\\\\\hline\n",
      "0.346 & 0.435 & 0.000 & 0.326 & 0.384 & 0.388 & 0.317 & 0.054\\\\\\hline\n",
      "963 & 378 & 114 & 585\\\\\\hline\n",
      "0.375 & 0.443 & 0.108 & 0.346 & 0.429 & 0.398 & 0.358 & 0.035\\\\\\hline\n",
      "963 & 416 & 319 & 547\\\\\\hline\n",
      "0.386 & 0.457 & 0.108 & 0.367 & 0.421 & 0.409 & 0.369 & 0.060\\\\\\hline\n",
      "963 & 427 & 334 & 536\\\\\\hline\n",
      "0.363 & 0.457 & 0.000 & 0.339 & 0.410 & 0.407 & 0.332 & 0.036\\\\\\hline\n",
      "963 & 402 & 156 & 561\\\\\\hline\n",
      "0.399 & 0.474 & 0.108 & 0.374 & 0.447 & 0.427 & 0.380 & 0.039\\\\\\hline\n",
      "963 & 438 & 376 & 525\\\\\\hline\n"
     ]
    }
   ],
   "source": [
    "#SysComb Experiments\n",
    "#Combine system 1-2, system 2-3, system 1-3, system 1,2,3\n",
    "#WCombAvg, WCombMax, WcombTWVWeight\n",
    "#4x3x2=24 experiments\n",
    "import copy\n",
    "\n",
    "\n",
    "#T1\n",
    "m_w =0.5\n",
    "w_w=0.5\n",
    "s2_w=0.5\n",
    "use_max=False;\n",
    "\"\"\"\n",
    "#T2\n",
    "use_max=True\n",
    "\n",
    "#T3\n",
    "\n",
    "m_w = 0.52\n",
    "w_w=0.46\n",
    "s2_w=0.47\n",
    "use_max=False;\n",
    "\"\"\"\n",
    "\n",
    "print_latex=True\n",
    "\n",
    "wl = read_posting_list(\"../lib/kws/word.xml\")\n",
    "normalize_list(wl,1)\n",
    "_ = gen_output(wl, \"../output/ref_word.xml\")\n",
    "_ = get_scores(\"ref_word\", print_latex)\n",
    "\n",
    "ml = read_posting_list(\"../lib/kws/morph.xml\")\n",
    "normalize_list(ml,1)\n",
    "_ = gen_output(ml, \"../output/ref_morph.xml\")\n",
    "_ = get_scores(\"ref_morph\", print_latex)\n",
    "\n",
    "s2l = read_posting_list(\"../lib/kws/word-sys2.xml\")\n",
    "normalize_list(s2l,1)\n",
    "_ = gen_output(s2l, \"../output/ref_sys2.xml\")\n",
    "_ = get_scores(\"ref_sys2\",print_latex)\n",
    "\n",
    "wml = sys_comb(copy.deepcopy(wl), copy.deepcopy(ml), w_w/(m_w+w_w), m_w/(m_w+w_w), use_max)\n",
    "_ = gen_output(wml, \"../output/ref_word_morph.xml\")\n",
    "_ = get_scores(\"ref_word_morph\",print_latex)\n",
    "\n",
    "ms2l = sys_comb(copy.deepcopy(ml), copy.deepcopy(s2l), m_w/(m_w+s2_w), s2_w/(m_w+s2_w), use_max)\n",
    "_ = gen_output(ms2l, \"../output/ref_morph_sys2.xml\")\n",
    "_ = get_scores(\"ref_morph_sys2\",print_latex)\n",
    "\n",
    "ws2l = sys_comb(copy.deepcopy(wl), copy.deepcopy(s2l), w_w/(w_w+s2_w), s2_w/(w_w+s2_w), use_max)\n",
    "_ = gen_output(ws2l, \"../output/ref_word_sys2.xml\")\n",
    "_ = get_scores(\"ref_word_sys2\",print_latex)\n",
    "\n",
    "wms2l = sys_comb(copy.deepcopy(wml), copy.deepcopy(s2l), (m_w+w_w)/(m_w+w_w+s2_w), s2_w/(m_w+w_w+s2_w), use_max)\n",
    "_ = gen_output(wms2l, \"../output/ref_word_morph_sys2.xml\")\n",
    "_ = get_scores(\"ref_word_morph_sys2\",print_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all (488) - TWV:0.4105\n",
      "iv (388) - TWV:0.4684\n",
      "oov (100) - TWV:0.1859\n",
      "short (321) - TWV:0.3757\n",
      "long (167) - TWV:0.4773\n",
      "word (288) - TWV:0.4027\n",
      "phrase (200) - TWV:0.4216\n",
      "Threshold: 0.054\n",
      "Targets: 963 Correct: 492 False Alarms: 1761 Miss: 471\n",
      "all (488) - TWV:0.4049\n",
      "iv (388) - TWV:0.4599\n",
      "oov (100) - TWV:0.1912\n",
      "short (321) - TWV:0.3567\n",
      "long (167) - TWV:0.4974\n",
      "word (288) - TWV:0.3853\n",
      "phrase (200) - TWV:0.4330\n",
      "Threshold: 0.055\n",
      "Targets: 963 Correct: 487 False Alarms: 1713 Miss: 476\n",
      "all (488) - TWV:0.3881\n",
      "iv (388) - TWV:0.4486\n",
      "oov (100) - TWV:0.1534\n",
      "short (321) - TWV:0.3423\n",
      "long (167) - TWV:0.4760\n",
      "word (288) - TWV:0.3748\n",
      "phrase (200) - TWV:0.4072\n",
      "Threshold: 0.066\n",
      "Targets: 963 Correct: 460 False Alarms: 1185 Miss: 503\n",
      "all (488) - TWV:0.4126\n",
      "iv (388) - TWV:0.4688\n",
      "oov (100) - TWV:0.1944\n",
      "short (321) - TWV:0.3690\n",
      "long (167) - TWV:0.4964\n",
      "word (288) - TWV:0.4013\n",
      "phrase (200) - TWV:0.4288\n",
      "Threshold: 0.04\n",
      "Targets: 963 Correct: 498 False Alarms: 2002 Miss: 465\n"
     ]
    }
   ],
   "source": [
    "#SysComb Experiments\n",
    "#Combine system 1-2, system 2-3, system 1-3, system 1,2,3\n",
    "#WCombAvg, WCombMax, WcombTWVWeight\n",
    "#4x3x2=24 experiments\n",
    "import copy\n",
    "\n",
    "\n",
    "#T1\n",
    "m_w =0.5\n",
    "w_w=0.5\n",
    "s2_w=0.5\n",
    "use_max=False;\n",
    "\"\"\"\n",
    "#T2\n",
    "use_max=True\n",
    "\n",
    "#T3\n",
    "\n",
    "m_w = 0.3790\n",
    "w_w=0.3763\n",
    "s2_w=0.3739\n",
    "use_max=False;\n",
    "\"\"\"\n",
    "\n",
    "print_latex=False\n",
    "\"\"\"\n",
    "keyword_index = create_index(\"../lib/ctms/decode.ctm\", True, False)\n",
    "query_list = get_queries(\"../lib/kws/queries.xml\")\n",
    "query_results_oov_c = search_index_oov_c(query_list, keyword_index, g_dict, 30, 7)\n",
    "kst_list(query_results_oov_c)\n",
    "normalize_list(query_results_oov_c,1)\n",
    "_ = gen_output(query_results_oov_c, \"../output/decode_oov.xml\")\n",
    "_ = get_scores(\"decode_oov\")\n",
    "\n",
    "keyword_index_morph = create_index(\"../lib/ctms/decode-morph.ctm\", True, False)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\",True)\n",
    "query_results_oov_morph_c = search_index_oov_c(query_list_morph, keyword_index_morph, g_dict, 30, 5)\n",
    "kst_list(query_results_oov_morph_c)\n",
    "normalize_list(query_results_oov_morph_c,1)\n",
    "_ = gen_output(query_results_oov_morph_c, \"../output/decode_morph_oov.xml\")\n",
    "_ = get_scores(\"decode_morph_oov\")\n",
    "\n",
    "kw_idx_morph_manual = create_index(\"../lib/ctms/decode.ctm\", True, True)\n",
    "query_list_morph = get_queries(\"../lib/kws/queries.xml\", True)\n",
    "query_results_morph_manual_oov = search_index_oov_c(query_list_morph, kw_idx_morph_manual, g_dict, 30, 5)\n",
    "kst_list(query_results_morph_manual_oov)\n",
    "normalize_list(query_results_morph_manual_oov,1)\n",
    "_ = gen_output(query_results_morph_manual_oov, \"../output/decode_morph_manual_oov.xml\")\n",
    "_ = get_scores(\"decode_morph_manual_oov\")\n",
    "\n",
    "wl = query_results_oov_c\n",
    "ml = query_results_oov_morph_c\n",
    "s2l = query_results_morph_manual_oov\n",
    "\"\"\"\n",
    "wml = sys_comb(copy.deepcopy(wl), copy.deepcopy(ml), w_w/(m_w+w_w), m_w/(m_w+w_w), use_max)\n",
    "_ = gen_output(wml, \"../output/ref_word_morph.xml\")\n",
    "_ = get_scores(\"ref_word_morph\",print_latex)\n",
    "\n",
    "ms2l = sys_comb(copy.deepcopy(ml), copy.deepcopy(s2l), m_w/(m_w+s2_w), s2_w/(m_w+s2_w), use_max)\n",
    "_ = gen_output(ms2l, \"../output/ref_morph_sys2.xml\")\n",
    "_ = get_scores(\"ref_morph_sys2\",print_latex)\n",
    "\n",
    "ws2l = sys_comb(copy.deepcopy(wl), copy.deepcopy(s2l), w_w/(w_w+s2_w), s2_w/(w_w+s2_w), use_max)\n",
    "_ = gen_output(ws2l, \"../output/ref_word_sys2.xml\")\n",
    "_ = get_scores(\"ref_word_sys2\",print_latex)\n",
    "\n",
    "wms2l = sys_comb(copy.deepcopy(wml), copy.deepcopy(s2l), (m_w+w_w)/(m_w+w_w+s2_w), s2_w/(m_w+w_w+s2_w), use_max)\n",
    "_ = gen_output(wms2l, \"../output/ref_word_morph_sys2.xml\")\n",
    "_ = get_scores(\"ref_word_morph_sys2\",print_latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average\n",
    "\n",
    "0.410 & 0.468 & 0.186 & 0.376 & 0.477 & 0.422 & 0.403 & 0.054\\\\\\hline\n",
    "963 & 492 & 1761 & 471\\\\\\hline\n",
    "0.405 & 0.460 & 0.191 & 0.357 & 0.497 & 0.433 & 0.385 & 0.055\\\\\\hline\n",
    "963 & 487 & 1713 & 476\\\\\\hline\n",
    "0.388 & 0.449 & 0.153 & 0.342 & 0.476 & 0.407 & 0.375 & 0.066\\\\\\hline\n",
    "963 & 460 & 1185 & 503\\\\\\hline\n",
    "0.413 & 0.469 & 0.194 & 0.369 & 0.496 & 0.429 & 0.401 & 0.040\\\\\\hline\n",
    "963 & 498 & 2002 & 465\\\\\\hline\n",
    "\n",
    "max\n",
    "\n",
    "0.406 & 0.463 & 0.186 & 0.369 & 0.477 & 0.421 & 0.396 & 0.105\\\\\\hline\n",
    "963 & 492 & 1761 & 471\\\\\\hline\n",
    "0.404 & 0.457 & 0.198 & 0.355 & 0.498 & 0.434 & 0.384 & 0.063\\\\\\hline\n",
    "963 & 487 & 1713 & 476\\\\\\hline\n",
    "0.383 & 0.445 & 0.143 & 0.336 & 0.475 & 0.404 & 0.369 & 0.079\\\\\\hline\n",
    "963 & 460 & 1185 & 503\\\\\\hline\n",
    "0.409 & 0.462 & 0.204 & 0.365 & 0.495 & 0.428 & 0.397 & 0.105\\\\\\hline\n",
    "963 & 498 & 2002 & 465\\\\\\hline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
